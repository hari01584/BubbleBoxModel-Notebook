{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train record files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_train model! (run command on command prompt as it will take lots of time_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">python model_main_tf2.py --model_dir=\"dataset-sampling\\models\\my_ssd_resnet50_v1_fpn\" --pipeline_config_path=\"dataset-sampling\\models\\my_ssd_resnet50_v1_fpn\\pipeline.config\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_export model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">python .\\exporter_main_v2.py --input_type image_tensor --pipeline_config_path dataset-sampling\\models\\my_ssd_resnet50_v1_fpn\\pipeline.config --trained_checkpoint_dir dataset-sampling\\models\\my_ssd_resnet50_v1_fpn --output_directory .\\dataset-sampling\\exported-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset-raw\"\n",
    "DATASET_SAMPLING = \"dataset-sampling\"\n",
    "NUMBER_IMAGE_TEST = 50\n",
    "HEIGHT_CV2_IMAGE_RESIZE = 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def cv_imread(file_path):\n",
    "    # Load image https://stackoverflow.com/a/54491104/4555317\n",
    "    cv_img = cv2.imdecode(np.fromfile(file_path, dtype=np.uint8), 1)\n",
    "    return cv_img\n",
    "\n",
    "def ResizeWithAspectRatio(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
    "    # stackoverflow copied resize cv2 image\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    if width is None:\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "    else:\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    return cv2.resize(image, dim, interpolation=inter)\n",
    "\n",
    "def quickShow(wt, img):\n",
    "    resize = ResizeWithAspectRatio(img, height=HEIGHT_CV2_IMAGE_RESIZE)\n",
    "    cv2.imshow(wt, resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict from saved-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...Done! Took 8.253122329711914 seconds\n"
     ]
    }
   ],
   "source": [
    "import time, os\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "PATH_TO_SAVED_MODEL = os.path.join(DATASET_SAMPLING,\"exported-models\",\"saved_model\")\n",
    "\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# Load saved model and build the detection function\n",
    "model = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "detect_fn = model.signatures['serving_default']\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = {1: {\"id\": 1, \"name\": \"bubble\"}}\n",
    "import random\n",
    "def datasetImage():\n",
    "    root = DATASET_PATH\n",
    "    folders = random.sample(os.listdir(root), 1)\n",
    "    for folder in folders:\n",
    "        b = os.path.join(root, folder)\n",
    "        f = random.choice(os.listdir(b))\n",
    "        ff = os.path.join(b, f)\n",
    "        print(ff)\n",
    "        return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset-raw\\The Inner Palace Tale Of A Villainess Noble Girl\\chapter-6_21.jpg\n",
      "dataset-raw\\I DonΓÇÖT Want A Harem!\\chapter-30_42.jpg\n",
      "dataset-raw\\Invasion Of The Immortal Emperor\\chapter-25_6.jpg\n",
      "dataset-raw\\Macho Γÿà Max\\chapter-8_22.jpg\n",
      "dataset-raw\\Isekai Kenkokuki\\chapter-51.2_19.jpg\n",
      "dataset-raw\\The Precious Sister Of The Villainous Grand Duke\\chapter-59_1.jpg\n",
      "dataset-raw\\Futari Solo Camp\\chapter-26_13.jpg\n",
      "dataset-raw\\My Home Hero\\chapter-99_14.jpg\n",
      "dataset-raw\\Trap City\\chapter-15_29.jpg\n",
      "dataset-raw\\Argonavis From Bang Dream! Comics\\chapter-14_4.jpg\n",
      "dataset-raw\\Spirit KingΓÇÖS Rules\\chapter-60_5.jpg\n",
      "dataset-raw\\Solo Spell Caster\\chapter-75_7.jpg\n",
      "dataset-raw\\The Elusive Samurai\\chapter-45_8.jpg\n",
      "dataset-raw\\Historie\\chapter-120_9.jpg\n",
      "dataset-raw\\The Sacrificial Princess\\chapter-41_39.jpg\n",
      "dataset-raw\\Tonari No Seki No Hen Na Senpai\\chapter-7_9.jpg\n",
      "dataset-raw\\Falling All In You\\chapter-100_26.jpg\n",
      "dataset-raw\\Tensei Kizoku No Isekai Boukenroku ~Jichou Wo Shiranai Kamigami No Shito~\\chapter-38_30.jpg\n",
      "dataset-raw\\Undine Of The Desert World\\chapter-30_2.jpg\n",
      "dataset-raw\\Jabberwocky\\chapter-23_11.jpg\n",
      "dataset-raw\\Martial Master\\chapter-548_7.jpg\n",
      "dataset-raw\\Your True Color\\chapter-16.5_2.jpg\n",
      "dataset-raw\\Masamune-Kun's ReΓùïΓùïΓùï\\chapter-12_1.jpg\n",
      "dataset-raw\\I Love You, As A Friend\\chapter-17_21.jpg\n",
      "dataset-raw\\Mo Shou Jian Sheng\\chapter-98_27.jpg\n",
      "dataset-raw\\Falling All In You\\chapter-100_22.jpg\n",
      "dataset-raw\\The Vengeance\\chapter-155_5.jpg\n",
      "dataset-raw\\Kuroko No Basuke - Replace Plus\\chapter-31_6.jpg\n",
      "dataset-raw\\Takanozomi No Koishite Warui Ka Yo\\chapter-35_5.jpg\n",
      "dataset-raw\\My Lover Has Powers!\\chapter-1_9.jpg\n",
      "dataset-raw\\Yuki Nee-Chan No Kan-Nou Gokko\\chapter-18_5.jpg\n",
      "dataset-raw\\Reborn 80,000 Years\\chapter-277_2.jpg\n",
      "dataset-raw\\Peerless Martial Spirit\\chapter-125_2.jpg\n",
      "dataset-raw\\Good Deeds Of Kane Of Old Guy\\chapter-27_1.jpg\n",
      "dataset-raw\\No Doubt In Us\\chapter-173.2_4.jpg\n",
      "dataset-raw\\Class Teni De Ore Dake Haburaretara, Doukyuu Harem Tsukuru Koto Ni Shita\\chapter-15_9.jpg\n",
      "dataset-raw\\Phantom Seer\\chapter-29_1.jpg\n",
      "dataset-raw\\The Detective Of Muiella\\chapter-163_13.jpg\n",
      "dataset-raw\\I'm My Household Girlfriend\\chapter-6_15.jpg\n",
      "dataset-raw\\Game Obu Familia - Family Senki\\chapter-37_9.jpg\n",
      "dataset-raw\\Beauty And The Beast Girl\\chapter-49_9.jpg\n",
      "dataset-raw\\Build Up\\chapter-39_76.jpg\n",
      "dataset-raw\\Peerless Martial Spirit\\chapter-125_3.jpg\n",
      "dataset-raw\\Inubaka\\chapter-184_3.jpg\n",
      "dataset-raw\\A Villain Is A Good Match For A Tyrant\\chapter-88_122.jpg\n",
      "dataset-raw\\Boku No Kanojo Sensei\\chapter-33_25.jpg\n",
      "dataset-raw\\Tensei Shitara Slime Datta Ken\\chapter-91_35.jpg\n",
      "dataset-raw\\The King's Avatar\\chapter-77_10.jpg\n",
      "dataset-raw\\Kiss Ni Oborete Aisarete\\chapter-8_15.jpg\n",
      "dataset-raw\\Sijin\\chapter-205.5_17.jpg\n"
     ]
    }
   ],
   "source": [
    "IMAGE_PATHS = []\n",
    "for i in range(NUMBER_IMAGE_TEST):\n",
    "    IMAGE_PATHS.append(datasetImage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for dataset-raw\\The Inner Palace Tale Of A Villainess Noble Girl\\chapter-6_21.jpg... Done\n",
      "Running inference for dataset-raw\\I DonΓÇÖT Want A Harem!\\chapter-30_42.jpg... Done\n",
      "Running inference for dataset-raw\\Invasion Of The Immortal Emperor\\chapter-25_6.jpg... Done\n",
      "Running inference for dataset-raw\\Macho Γÿà Max\\chapter-8_22.jpg... Done\n",
      "Running inference for dataset-raw\\Isekai Kenkokuki\\chapter-51.2_19.jpg... Done\n",
      "Running inference for dataset-raw\\The Precious Sister Of The Villainous Grand Duke\\chapter-59_1.jpg... Done\n",
      "Running inference for dataset-raw\\Futari Solo Camp\\chapter-26_13.jpg... Done\n",
      "Running inference for dataset-raw\\My Home Hero\\chapter-99_14.jpg... Done\n",
      "Running inference for dataset-raw\\Trap City\\chapter-15_29.jpg... Done\n",
      "Running inference for dataset-raw\\Argonavis From Bang Dream! Comics\\chapter-14_4.jpg... Done\n",
      "Running inference for dataset-raw\\Spirit KingΓÇÖS Rules\\chapter-60_5.jpg... Done\n",
      "Running inference for dataset-raw\\Solo Spell Caster\\chapter-75_7.jpg... Done\n",
      "Running inference for dataset-raw\\The Elusive Samurai\\chapter-45_8.jpg... Done\n",
      "Running inference for dataset-raw\\Historie\\chapter-120_9.jpg... Done\n",
      "Running inference for dataset-raw\\The Sacrificial Princess\\chapter-41_39.jpg... Done\n",
      "Running inference for dataset-raw\\Tonari No Seki No Hen Na Senpai\\chapter-7_9.jpg... Done\n",
      "Running inference for dataset-raw\\Falling All In You\\chapter-100_26.jpg... Done\n",
      "Running inference for dataset-raw\\Tensei Kizoku No Isekai Boukenroku ~Jichou Wo Shiranai Kamigami No Shito~\\chapter-38_30.jpg... Done\n",
      "Running inference for dataset-raw\\Undine Of The Desert World\\chapter-30_2.jpg... Done\n",
      "Running inference for dataset-raw\\Jabberwocky\\chapter-23_11.jpg... Done\n",
      "Running inference for dataset-raw\\Martial Master\\chapter-548_7.jpg... Done\n",
      "Running inference for dataset-raw\\Your True Color\\chapter-16.5_2.jpg... Done\n",
      "Running inference for dataset-raw\\Masamune-Kun's ReΓùïΓùïΓùï\\chapter-12_1.jpg... Done\n",
      "Running inference for dataset-raw\\I Love You, As A Friend\\chapter-17_21.jpg... Done\n",
      "Running inference for dataset-raw\\Mo Shou Jian Sheng\\chapter-98_27.jpg... Done\n",
      "Running inference for dataset-raw\\Falling All In You\\chapter-100_22.jpg... Done\n",
      "Running inference for dataset-raw\\The Vengeance\\chapter-155_5.jpg... Done\n",
      "Running inference for dataset-raw\\Kuroko No Basuke - Replace Plus\\chapter-31_6.jpg... Done\n",
      "Running inference for dataset-raw\\Takanozomi No Koishite Warui Ka Yo\\chapter-35_5.jpg... Done\n",
      "Running inference for dataset-raw\\My Lover Has Powers!\\chapter-1_9.jpg... Done\n",
      "Running inference for dataset-raw\\Yuki Nee-Chan No Kan-Nou Gokko\\chapter-18_5.jpg... Done\n",
      "Running inference for dataset-raw\\Reborn 80,000 Years\\chapter-277_2.jpg... Done\n",
      "Running inference for dataset-raw\\Peerless Martial Spirit\\chapter-125_2.jpg... Done\n",
      "Running inference for dataset-raw\\Good Deeds Of Kane Of Old Guy\\chapter-27_1.jpg... Done\n",
      "Running inference for dataset-raw\\No Doubt In Us\\chapter-173.2_4.jpg... Done\n",
      "Running inference for dataset-raw\\Class Teni De Ore Dake Haburaretara, Doukyuu Harem Tsukuru Koto Ni Shita\\chapter-15_9.jpg... Done\n",
      "Running inference for dataset-raw\\Phantom Seer\\chapter-29_1.jpg... Done\n",
      "Running inference for dataset-raw\\The Detective Of Muiella\\chapter-163_13.jpg... Done\n",
      "Running inference for dataset-raw\\I'm My Household Girlfriend\\chapter-6_15.jpg... Done\n",
      "Running inference for dataset-raw\\Game Obu Familia - Family Senki\\chapter-37_9.jpg... Done\n",
      "Running inference for dataset-raw\\Beauty And The Beast Girl\\chapter-49_9.jpg... Done\n",
      "Running inference for dataset-raw\\Build Up\\chapter-39_76.jpg... Done\n",
      "Running inference for dataset-raw\\Peerless Martial Spirit\\chapter-125_3.jpg... Done\n",
      "Running inference for dataset-raw\\Inubaka\\chapter-184_3.jpg... Done\n",
      "Running inference for dataset-raw\\A Villain Is A Good Match For A Tyrant\\chapter-88_122.jpg... Done\n",
      "Running inference for dataset-raw\\Boku No Kanojo Sensei\\chapter-33_25.jpg... Done\n",
      "Running inference for dataset-raw\\Tensei Shitara Slime Datta Ken\\chapter-91_35.jpg... Done\n",
      "Running inference for dataset-raw\\The King's Avatar\\chapter-77_10.jpg... Done\n",
      "Running inference for dataset-raw\\Kiss Ni Oborete Aisarete\\chapter-8_15.jpg... Done\n",
      "Running inference for dataset-raw\\Sijin\\chapter-205.5_17.jpg... Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import cv2\n",
    "#warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "\n",
    "for image_path in IMAGE_PATHS:\n",
    "\n",
    "    print('Running inference for {}... '.format(image_path), end='')\n",
    "\n",
    "    image_np = cv_imread(image_path)\n",
    "\n",
    "    # Things to try:\n",
    "    # Flip horizontally\n",
    "    # image_np = np.fliplr(image_np).copy()\n",
    "\n",
    "    # Convert image to grayscale\n",
    "    # image_np = np.tile(\n",
    "    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image_np)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # input_tensor = np.expand_dims(image_np, 0)\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                   for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_detections,\n",
    "          detections['detection_boxes'],\n",
    "          detections['detection_classes'],\n",
    "          detections['detection_scores'],\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=200,\n",
    "          min_score_thresh=.30,\n",
    "          agnostic_mode=False)\n",
    "    \n",
    "    print('Done')\n",
    "    bgr_img = cv2.cvtColor(image_np_with_detections, cv2.COLOR_RGB2BGR)\n",
    "    quickShow(\"im\", bgr_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
